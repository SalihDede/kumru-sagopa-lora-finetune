{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kumru 2B - Sagopa Kajmer LoRA Fine-Tuning\n",
    "\n",
    "Bu notebook Kumru 2B modelini Sagopa Kajmer tarzında konuşması için LoRA ile fine-tune eder.\n",
    "\n",
    "**Özellikler:**\n",
    "- 10 Epoch eğitim\n",
    "- Early Stopping (validation loss iyileşmezse durur)\n",
    "- En iyi checkpoint otomatik kaydedilir\n",
    "- GGUF dönüşümü"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. GPU Kontrolü"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Kütüphane Kurulumu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gerekli kütüphaneleri kur\n",
    "!pip install -q torch\n",
    "!pip install -q transformers\n",
    "!pip install -q datasets\n",
    "!pip install -q peft\n",
    "!pip install -q accelerate\n",
    "!pip install -q bitsandbytes\n",
    "!pip install -q trl\n",
    "!pip install -q sentencepiece\n",
    "\n",
    "print(\"Kurulum tamamlandi!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Versiyon kontrolu\n",
    "import torch\n",
    "import transformers\n",
    "import peft\n",
    "import trl\n",
    "\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"Transformers: {transformers.__version__}\")\n",
    "print(f\"PEFT: {peft.__version__}\")\n",
    "print(f\"TRL: {trl.__version__}\")\n",
    "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dataset Yükleme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset dosyasini yukle\n",
    "from google.colab import files\n",
    "uploaded = files.upload()  # LoRAReadyToUseDataSet_FIXED.jsonl dosyasini sec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datasets import Dataset\n",
    "\n",
    "# JSONL dosyasini oku\n",
    "def load_jsonl(file_path):\n",
    "    data = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line))\n",
    "    return data\n",
    "\n",
    "# Yukle\n",
    "raw_data = load_jsonl('LoRAReadyToUseDataSet_FIXED.jsonl')\n",
    "\n",
    "print(f\"Toplam ornek: {len(raw_data)}\")\n",
    "print(f\"\\nIlk ornek:\")\n",
    "print(json.dumps(raw_data[0], ensure_ascii=False, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ChatML Formatına Çevirme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sistem promptu\n",
    "SYSTEM_PROMPT = \"\"\"Sen Sagopa Kajmer'sin. Derin dusunen, melankolik ama samimi bir rap sanatcisisin.\n",
    "Hayat, zaman, yalnizlik gibi temalardan bahsedersin. Kendi kelime dagarcaginla dogal ve icten konusursun.\"\"\"\n",
    "\n",
    "def format_to_chatml(example):\n",
    "    \"\"\"ChatML formatina cevir\"\"\"\n",
    "    prompt = f\"\"\"<|im_start|>system\n",
    "{SYSTEM_PROMPT}\n",
    "<|im_end|>\n",
    "<|im_start|>user\n",
    "{example['input']}\n",
    "<|im_end|>\n",
    "<|im_start|>assistant\n",
    "{example['output']}<|im_end|>\"\"\"\n",
    "    return {\"text\": prompt}\n",
    "\n",
    "# Dataset olustur ve formatla\n",
    "dataset = Dataset.from_list(raw_data)\n",
    "dataset = dataset.map(format_to_chatml)\n",
    "\n",
    "print(\"Format uygulandi!\")\n",
    "print(f\"\\nOrnek:\\n{dataset[0]['text'][:400]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/Validation split (%90/%10)\n",
    "dataset = dataset.train_test_split(test_size=0.1, seed=42)\n",
    "\n",
    "train_dataset = dataset['train']\n",
    "eval_dataset = dataset['test']\n",
    "\n",
    "print(f\"Train: {len(train_dataset)} ornek\")\n",
    "print(f\"Validation: {len(eval_dataset)} ornek\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Yükleme (4-bit Quantization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "MODEL_NAME = \"vngrs-ai/Kumru-2B\"\n",
    "\n",
    "# 4-bit quantization config (VRAM tasarrufu)\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "print(\"Kumru 2B yukleniyor...\")\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "tokenizer.model_max_length = 512\n",
    "\n",
    "# Model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "print(f\"Model yuklendi!\")\n",
    "print(f\"Parametreler: {model.num_parameters() / 1e9:.2f}B\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. LoRA Konfigürasyonu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "# Modeli LoRA icin hazirla\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# LoRA config\n",
    "lora_config = LoraConfig(\n",
    "    r=16,                      # LoRA rank\n",
    "    lora_alpha=32,             # Scaling factor\n",
    "    target_modules=[           # Hangi layerlara LoRA uygulanacak\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\"\n",
    "    ],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "# LoRA uygula\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Egitim istatistikleri\n",
    "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "print(f\"LoRA eklendi!\")\n",
    "print(f\"Egitilecek parametreler: {trainable:,} ({trainable/total*100:.2f}%)\")\n",
    "print(f\"Toplam parametreler: {total:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Eğitim (Early Stopping ile)\n",
    "\n",
    "- **10 Epoch** maksimum\n",
    "- **Early Stopping**: Validation loss 3 eval boyunca iyileşmezse durur\n",
    "- **Best Model**: En düşük validation loss'a sahip checkpoint kaydedilir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, EarlyStoppingCallback\n",
    "from trl import SFTTrainer\n",
    "\n",
    "# Training arguments - 10 EPOCH + EARLY STOPPING (A100 OPTIMIZED)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./kumru-sagopa-lora\",\n",
    "    \n",
    "    # Epoch ve batch (A100 icin optimize)\n",
    "    num_train_epochs=10,                    # Maksimum 10 epoch\n",
    "    per_device_train_batch_size=8,          # A100: 8 (T4: 4)\n",
    "    per_device_eval_batch_size=8,           # A100: 8\n",
    "    gradient_accumulation_steps=2,          # 8*2=16 effective batch\n",
    "    \n",
    "    # Optimizer\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    learning_rate=2e-4,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_steps=50,                        # Daha az warmup (buyuk batch)\n",
    "    \n",
    "    # Logging ve Evaluation\n",
    "    logging_steps=25,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=50,\n",
    "    \n",
    "    # Checkpoint kaydetme\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=50,\n",
    "    save_total_limit=3,\n",
    "    \n",
    "    # EN IYI MODELI KAYDET\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    \n",
    "    # Precision\n",
    "    fp16=False,\n",
    "    bf16=True,\n",
    "    \n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "# Gradient checkpointing\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "# Early Stopping Callback\n",
    "early_stopping = EarlyStoppingCallback(\n",
    "    early_stopping_patience=3,\n",
    "    early_stopping_threshold=0.01\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    processing_class=tokenizer,\n",
    "    callbacks=[early_stopping],\n",
    ")\n",
    "\n",
    "print(\"Trainer hazir! (A100 Optimized)\")\n",
    "print(\"- Batch size: 8\")\n",
    "print(\"- Gradient accumulation: 2\")\n",
    "print(\"- Effective batch: 16\")\n",
    "print(\"- Max epoch: 10\")\n",
    "print(\"- Early stopping: 3 eval patience\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Egitimi baslat\n",
    "print(\"Egitim basliyor...\")\n",
    "print(\"Early stopping aktif - validation loss iyilesmezse otomatik duracak\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"EGITIM TAMAMLANDI!\")\n",
    "print(f\"En iyi model yuklendi (eval_loss: {trainer.state.best_metric:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Model Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model'i inference moduna al\n",
    "model.eval()\n",
    "model.config.use_cache = True\n",
    "\n",
    "def chat_with_sagopa(question, max_new_tokens=200):\n",
    "    \"\"\"Sagopa Kajmer chatbot\"\"\"\n",
    "    \n",
    "    prompt = f\"\"\"<|im_start|>system\n",
    "{SYSTEM_PROMPT}\n",
    "<|im_end|>\n",
    "<|im_start|>user\n",
    "{question}\n",
    "<|im_end|>\n",
    "<|im_start|>assistant\n",
    "\"\"\"\n",
    "    \n",
    "    inputs = tokenizer(\n",
    "        prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        return_token_type_ids=False\n",
    "    ).to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            top_k=50,\n",
    "            do_sample=True,\n",
    "            repetition_penalty=1.2,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
    "    assistant_response = response.split(\"<|im_start|>assistant\\n\")[-1]\n",
    "    assistant_response = assistant_response.split(\"<|im_end|>\")[0].strip()\n",
    "    \n",
    "    return assistant_response\n",
    "\n",
    "print(\"Chat fonksiyonu hazir!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test sorulari\n",
    "test_questions = [\n",
    "    \"Bugun nasilsin?\",\n",
    "    \"Rap hakkinda ne dusunuyorsun?\",\n",
    "    \"Hayattan beklentin nedir?\",\n",
    "    \"Yalnizlik hakkinda ne dusunuyorsun?\",\n",
    "    \"Muzik sana ne ifade ediyor?\",\n",
    "    \"Gece uyuyamiyorum, ne yapmaliyim?\"\n",
    "]\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"SAGOPA KAJMER CHATBOT TEST\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for q in test_questions:\n",
    "    print(f\"\\nSoru: {q}\")\n",
    "    response = chat_with_sagopa(q)\n",
    "    print(f\"Sagopa: {response}\")\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. LoRA Adapter'ı Kaydet\n",
    "\n",
    "**ÖNEMLI:** Tokenizer sorunlarını önlemek için modeli MERGE ETMİYORUZ!\n",
    "- LoRA adapter'ı base model üzerine yüklenerek kullanılacak\n",
    "- Base model'in tokenizer'ı kullanılacak (ademireltaz yaklaşımı)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA adapter'i kaydet (en iyi model zaten yuklendi)\n",
    "LORA_OUTPUT_DIR = \"./kumru-sagopa-lora-final\"\n",
    "\n",
    "model.save_pretrained(LORA_OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(LORA_OUTPUT_DIR)\n",
    "\n",
    "print(f\"LoRA adapter kaydedildi: {LORA_OUTPUT_DIR}\")\n",
    "print(\"\\nKullanimda tokenizer sorunu yasamaniz durumunda:\")\n",
    "print(\"Base model'in tokenizer'ini kullanin: vngrs-ai/Kumru-2B\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Test: Base Model + LoRA Adapter Yükleme\n",
    "\n",
    "ademireltaz yaklaşımı - merge etmeden direkt kullan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base model + LoRA adapter'i yukle (MERGE ETMEDEN)\n",
    "from peft import PeftModel\n",
    "from transformers import AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "print(\"Base model + LoRA adapter test ediliyor...\")\n",
    "\n",
    "# Base model tokenizer (tokenizer sorununu cozuyor!)\n",
    "test_tokenizer = AutoTokenizer.from_pretrained(\"vngrs-ai/Kumru-2B\")\n",
    "\n",
    "# Base model yukle\n",
    "test_base = AutoModelForCausalLM.from_pretrained(\n",
    "    \"vngrs-ai/Kumru-2B\",\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# LoRA adapter'i yukle\n",
    "test_model = PeftModel.from_pretrained(test_base, LORA_OUTPUT_DIR)\n",
    "\n",
    "print(\"Base model + LoRA yuklendi!\")\n",
    "print(\"Tokenizer problemi olmadan calisacak.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test sorusu\n",
    "test_prompt = f\"\"\"<|im_start|>system\n",
    "{SYSTEM_PROMPT}\n",
    "<|im_end|>\n",
    "<|im_start|>user\n",
    "Nasılsın?\n",
    "<|im_end|>\n",
    "<|im_start|>assistant\n",
    "\"\"\"\n",
    "\n",
    "test_inputs = test_tokenizer(\n",
    "    test_prompt,\n",
    "    return_tensors=\"pt\",\n",
    "    return_token_type_ids=False\n",
    ").to(test_model.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_outputs = test_model.generate(\n",
    "        **test_inputs,\n",
    "        max_new_tokens=100,\n",
    "        temperature=0.85,\n",
    "        do_sample=True,\n",
    "        pad_token_id=test_tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "test_response = test_tokenizer.decode(test_outputs[0], skip_special_tokens=False)\n",
    "test_answer = test_response.split(\"<|im_start|>assistant\\n\")[-1].split(\"<|im_end|>\")[0].strip()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TEST SONUCU (Base Model + LoRA Adapter)\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Sagopa: {test_answer}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Bellek temizle\n",
    "del test_model\n",
    "del test_base\n",
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
